{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements / ToDo\n",
    "[x]  train/test accuracy total + Fizz/Buzz/FizzBuzz separately    \n",
    "[x]  graphs for different hyperparameter options (do graphs)    \n",
    "[x]  try different learning algorithms    \n",
    "[ ]  include best setting in report    \n",
    "[x]  add main.py that creates output.csv    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Based FizzBuzz Function [Software 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/wolterlw/miniconda3/lib/python3.6/site-packages (4.19.4)\n",
      "Requirement already satisfied: hyperopt in /home/wolterlw/miniconda3/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: future in /home/wolterlw/miniconda3/lib/python3.6/site-packages (from hyperopt) (0.16.0)\n",
      "Requirement already satisfied: numpy in /home/wolterlw/miniconda3/lib/python3.6/site-packages (from hyperopt) (1.14.3)\n",
      "Requirement already satisfied: scipy in /home/wolterlw/miniconda3/lib/python3.6/site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: networkx in /home/wolterlw/miniconda3/lib/python3.6/site-packages (from hyperopt) (2.1)\n",
      "Requirement already satisfied: pymongo in /home/wolterlw/miniconda3/lib/python3.6/site-packages (from hyperopt) (3.7.1)\n",
      "Requirement already satisfied: six in /home/wolterlw/miniconda3/lib/python3.6/site-packages (from hyperopt) (1.11.0)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/wolterlw/miniconda3/lib/python3.6/site-packages (from networkx->hyperopt) (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolterlw/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import set_random_seed\n",
    "%matplotlib inline\n",
    "\n",
    "set_random_seed(574)\n",
    "np.random.seed(574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fizzbuzz(n):\n",
    "    # Logic Explanation\n",
    "    if n % 3 == 0 and n % 5 == 0:\n",
    "        return 'fizzbuzz'\n",
    "    elif n % 3 == 0:\n",
    "        return 'fizz'\n",
    "    elif n % 5 == 0:\n",
    "        return 'buzz'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Datasets in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createInputCSV(start,end,filename):\n",
    "    \n",
    "    # Why list in Python?\n",
    "    inputData   = []\n",
    "    outputData  = []\n",
    "    \n",
    "    # Why do we need training Data?\n",
    "    for i in range(start,end):\n",
    "        inputData.append(i)\n",
    "        outputData.append(fizzbuzz(i))\n",
    "    \n",
    "    # Why Dataframe?\n",
    "    dataset = {}\n",
    "    dataset[\"input\"]  = inputData\n",
    "    dataset[\"label\"] = outputData\n",
    "    \n",
    "    # Writing to csv\n",
    "    pd.DataFrame(dataset).to_csv(filename, index=False)\n",
    "    \n",
    "    print(filename, \"Created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Input and Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processData(dataset):\n",
    "    \n",
    "    # Why do we have to process?\n",
    "    data   = dataset['input'].values\n",
    "    labels = dataset['label'].values\n",
    "    \n",
    "    processedData  = encodeData(data)\n",
    "    processedLabel = encodeLabel(labels)\n",
    "    \n",
    "    return processedData, processedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encodeData(data):\n",
    "    \n",
    "    processedData = []\n",
    "    \n",
    "    for dataInstance in data:\n",
    "        \n",
    "        # Why do we have number 10?\n",
    "        # to encode digits from 1 to 1000\n",
    "        processedData.append([dataInstance >> d & 1 for d in range(10)])\n",
    "    \n",
    "    return np.array(processedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def encodeLabel(labels):\n",
    "    \n",
    "    processedLabel = []\n",
    "    \n",
    "    for labelInstance in labels:\n",
    "        if(labelInstance == \"fizzbuzz\"):\n",
    "            # Fizzbuzz\n",
    "            processedLabel.append([3])\n",
    "        elif(labelInstance == \"fizz\"):\n",
    "            # Fizz\n",
    "            processedLabel.append([1])\n",
    "        elif(labelInstance == \"buzz\"):\n",
    "            # Buzz\n",
    "            processedLabel.append([2])\n",
    "        else:\n",
    "            # Other\n",
    "            processedLabel.append([0])\n",
    "\n",
    "    return np_utils.to_categorical(np.array(processedLabel),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop, SGD, Adam, Nadam\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': SGD,\n",
    "    'RMSprop': RMSprop,\n",
    "    'Adam': Adam,\n",
    "    'Nadam': Nadam\n",
    "}\n",
    "\n",
    "input_size = 10\n",
    "output_size = 4\n",
    "\n",
    "def get_model(params, verbose=False, optim_algo='RMSprop'):\n",
    "    \n",
    "    # Why do we need a model?\n",
    "    # For it to learn the mapping from input to output\n",
    "    \n",
    "    # Why use Dense layer and then activation?\n",
    "    # Dense layers contain the model coeficients that perform linear transformations on the input data\n",
    "    # Activations are needed to introduce nonliniarities (composition of multiple linear transformation is just another linear transformation)\n",
    "    \n",
    "    # Why use sequential model with layers?\n",
    "    # Because there is no need to use functional API as we have a linear computation graph\n",
    "    \n",
    "    assert len(params['hidden_layer_nodes']) == params['num_hidden'], \"specify layer size for each hidden layer\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(params['hidden_layer_nodes'][0], input_dim=input_size))\n",
    "    model.add(Activation(params['activation']))\n",
    "    # Why dropout?\n",
    "    # to avoid overfitting (avoiding nodes that don't compute anything useful)\n",
    "    model.add(Dropout(params['drop_out']))\n",
    "    \n",
    "    for i in range(1,params['num_hidden']):\n",
    "        model.add(Dense(params['hidden_layer_nodes'][i]))\n",
    "        model.add(Activation(params['activation']))\n",
    "        model.add(Dropout(params['drop_out']))\n",
    "    \n",
    "    model.add(Dense(output_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    # Why Softmax?\n",
    "    # to get probabilistic output\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    # Why use categorical_crossentropy?\n",
    "    # because it's differentiable generalization of logloss that works for n-dimensional probabilistic outputs\n",
    "    optimizer = optimizers[optim_algo](lr = params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Training and Testing Datafiles</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.csv Created!\n",
      "testing.csv Created!\n"
     ]
    }
   ],
   "source": [
    "# Create datafiles\n",
    "createInputCSV(101,1001,'training.csv')\n",
    "createInputCSV(1,101,'testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'drop_out':  0.339,\n",
    "    'hidden_layer_nodes': [245, 154, 67],\n",
    "    'num_hidden': 3,\n",
    "    'activation': 'selu',\n",
    "    'learning_rate': 0.00815\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(hyper_params, optim_algo='SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Run Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 720 samples, validate on 180 samples\n",
      "Epoch 1/1000\n",
      "720/720 [==============================] - 6s 9ms/step - loss: 1.6283 - acc: 0.3236 - val_loss: 1.2006 - val_acc: 0.5333\n",
      "Epoch 2/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.4782 - acc: 0.3875 - val_loss: 1.2275 - val_acc: 0.5333\n",
      "Epoch 3/1000\n",
      "720/720 [==============================] - 0s 139us/step - loss: 1.4422 - acc: 0.4222 - val_loss: 1.2001 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "720/720 [==============================] - 0s 128us/step - loss: 1.3927 - acc: 0.4208 - val_loss: 1.2091 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.3788 - acc: 0.4347 - val_loss: 1.2115 - val_acc: 0.5333\n",
      "Epoch 6/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.3805 - acc: 0.4431 - val_loss: 1.2186 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.3525 - acc: 0.4472 - val_loss: 1.1937 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.3424 - acc: 0.4306 - val_loss: 1.2146 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.4023 - acc: 0.4486 - val_loss: 1.2010 - val_acc: 0.5333\n",
      "Epoch 10/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.3552 - acc: 0.4208 - val_loss: 1.2116 - val_acc: 0.5333\n",
      "Epoch 11/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.3904 - acc: 0.4361 - val_loss: 1.1896 - val_acc: 0.5333\n",
      "Epoch 12/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.3447 - acc: 0.4431 - val_loss: 1.1948 - val_acc: 0.5333\n",
      "Epoch 13/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.3262 - acc: 0.4611 - val_loss: 1.1984 - val_acc: 0.5333\n",
      "Epoch 14/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.3068 - acc: 0.4486 - val_loss: 1.1893 - val_acc: 0.5333\n",
      "Epoch 15/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.3367 - acc: 0.4528 - val_loss: 1.1930 - val_acc: 0.5333\n",
      "Epoch 16/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.3277 - acc: 0.4319 - val_loss: 1.1856 - val_acc: 0.5333\n",
      "Epoch 17/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.3318 - acc: 0.4347 - val_loss: 1.1983 - val_acc: 0.5333\n",
      "Epoch 18/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.3430 - acc: 0.4347 - val_loss: 1.1780 - val_acc: 0.5333\n",
      "Epoch 19/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.2997 - acc: 0.4333 - val_loss: 1.1781 - val_acc: 0.5333\n",
      "Epoch 20/1000\n",
      "720/720 [==============================] - 0s 132us/step - loss: 1.3296 - acc: 0.4333 - val_loss: 1.1783 - val_acc: 0.5333\n",
      "Epoch 21/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.2796 - acc: 0.4486 - val_loss: 1.1798 - val_acc: 0.5333\n",
      "Epoch 22/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.3168 - acc: 0.4583 - val_loss: 1.1856 - val_acc: 0.5333\n",
      "Epoch 23/1000\n",
      "720/720 [==============================] - 0s 161us/step - loss: 1.3062 - acc: 0.4569 - val_loss: 1.1890 - val_acc: 0.5333\n",
      "Epoch 24/1000\n",
      "720/720 [==============================] - 0s 156us/step - loss: 1.3056 - acc: 0.4653 - val_loss: 1.1783 - val_acc: 0.5333\n",
      "Epoch 25/1000\n",
      "720/720 [==============================] - 0s 121us/step - loss: 1.2907 - acc: 0.4639 - val_loss: 1.1780 - val_acc: 0.5333\n",
      "Epoch 26/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.2659 - acc: 0.4458 - val_loss: 1.1823 - val_acc: 0.5333\n",
      "Epoch 27/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.2939 - acc: 0.4375 - val_loss: 1.1808 - val_acc: 0.5333\n",
      "Epoch 28/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.2927 - acc: 0.4417 - val_loss: 1.1844 - val_acc: 0.5333\n",
      "Epoch 29/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.2749 - acc: 0.4389 - val_loss: 1.1866 - val_acc: 0.5333\n",
      "Epoch 30/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.2818 - acc: 0.4778 - val_loss: 1.1774 - val_acc: 0.5333\n",
      "Epoch 31/1000\n",
      "720/720 [==============================] - 0s 123us/step - loss: 1.3172 - acc: 0.4333 - val_loss: 1.1728 - val_acc: 0.5333\n",
      "Epoch 32/1000\n",
      "720/720 [==============================] - 0s 142us/step - loss: 1.2700 - acc: 0.4389 - val_loss: 1.1779 - val_acc: 0.5333\n",
      "Epoch 33/1000\n",
      "720/720 [==============================] - 0s 159us/step - loss: 1.2637 - acc: 0.4597 - val_loss: 1.1743 - val_acc: 0.5333\n",
      "Epoch 34/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.2852 - acc: 0.4514 - val_loss: 1.1675 - val_acc: 0.5333\n",
      "Epoch 35/1000\n",
      "720/720 [==============================] - 0s 146us/step - loss: 1.2688 - acc: 0.4458 - val_loss: 1.1725 - val_acc: 0.5333\n",
      "Epoch 36/1000\n",
      "720/720 [==============================] - 0s 165us/step - loss: 1.2846 - acc: 0.4528 - val_loss: 1.1704 - val_acc: 0.5333\n",
      "Epoch 37/1000\n",
      "720/720 [==============================] - 0s 162us/step - loss: 1.2769 - acc: 0.4500 - val_loss: 1.1738 - val_acc: 0.5333\n",
      "Epoch 38/1000\n",
      "720/720 [==============================] - 0s 127us/step - loss: 1.2875 - acc: 0.4500 - val_loss: 1.1723 - val_acc: 0.5333\n",
      "Epoch 39/1000\n",
      "720/720 [==============================] - 0s 123us/step - loss: 1.2592 - acc: 0.4722 - val_loss: 1.1733 - val_acc: 0.5333\n",
      "Epoch 40/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.2550 - acc: 0.4708 - val_loss: 1.1694 - val_acc: 0.5333\n",
      "Epoch 41/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.2444 - acc: 0.4708 - val_loss: 1.1682 - val_acc: 0.5333\n",
      "Epoch 42/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.2583 - acc: 0.4639 - val_loss: 1.1668 - val_acc: 0.5333\n",
      "Epoch 43/1000\n",
      "720/720 [==============================] - 0s 110us/step - loss: 1.2761 - acc: 0.4431 - val_loss: 1.1720 - val_acc: 0.5333\n",
      "Epoch 44/1000\n",
      "720/720 [==============================] - 0s 127us/step - loss: 1.2403 - acc: 0.4778 - val_loss: 1.1707 - val_acc: 0.5333\n",
      "Epoch 45/1000\n",
      "720/720 [==============================] - 0s 160us/step - loss: 1.2580 - acc: 0.4472 - val_loss: 1.1745 - val_acc: 0.5333\n",
      "Epoch 46/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.2545 - acc: 0.4861 - val_loss: 1.1749 - val_acc: 0.5333\n",
      "Epoch 47/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.2613 - acc: 0.4764 - val_loss: 1.1731 - val_acc: 0.5333\n",
      "Epoch 48/1000\n",
      "720/720 [==============================] - 0s 133us/step - loss: 1.2609 - acc: 0.4764 - val_loss: 1.1709 - val_acc: 0.5333\n",
      "Epoch 49/1000\n",
      "720/720 [==============================] - 0s 127us/step - loss: 1.2706 - acc: 0.4667 - val_loss: 1.1657 - val_acc: 0.5333\n",
      "Epoch 50/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.2564 - acc: 0.4653 - val_loss: 1.1669 - val_acc: 0.5333\n",
      "Epoch 51/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.2717 - acc: 0.4639 - val_loss: 1.1673 - val_acc: 0.5333\n",
      "Epoch 52/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.2290 - acc: 0.4819 - val_loss: 1.1689 - val_acc: 0.5333\n",
      "Epoch 53/1000\n",
      "720/720 [==============================] - 0s 139us/step - loss: 1.2822 - acc: 0.4528 - val_loss: 1.1678 - val_acc: 0.5333\n",
      "Epoch 54/1000\n",
      "720/720 [==============================] - 0s 114us/step - loss: 1.2259 - acc: 0.4875 - val_loss: 1.1695 - val_acc: 0.5333\n",
      "Epoch 55/1000\n",
      "720/720 [==============================] - 0s 145us/step - loss: 1.2353 - acc: 0.4792 - val_loss: 1.1655 - val_acc: 0.5333\n",
      "Epoch 56/1000\n",
      "720/720 [==============================] - 0s 158us/step - loss: 1.2564 - acc: 0.4778 - val_loss: 1.1666 - val_acc: 0.5333\n",
      "Epoch 57/1000\n",
      "720/720 [==============================] - 0s 121us/step - loss: 1.2479 - acc: 0.4750 - val_loss: 1.1684 - val_acc: 0.5333\n",
      "Epoch 58/1000\n",
      "720/720 [==============================] - 0s 118us/step - loss: 1.2541 - acc: 0.4750 - val_loss: 1.1678 - val_acc: 0.5333\n",
      "Epoch 59/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.2338 - acc: 0.4958 - val_loss: 1.1658 - val_acc: 0.5333\n",
      "Epoch 60/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.2599 - acc: 0.4542 - val_loss: 1.1630 - val_acc: 0.5333\n",
      "Epoch 61/1000\n",
      "720/720 [==============================] - 0s 127us/step - loss: 1.2085 - acc: 0.4931 - val_loss: 1.1652 - val_acc: 0.5333\n",
      "Epoch 62/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.2745 - acc: 0.4611 - val_loss: 1.1650 - val_acc: 0.5333\n",
      "Epoch 63/1000\n",
      "720/720 [==============================] - 0s 135us/step - loss: 1.2308 - acc: 0.4667 - val_loss: 1.1657 - val_acc: 0.5333\n",
      "Epoch 64/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.2431 - acc: 0.4875 - val_loss: 1.1644 - val_acc: 0.5333\n",
      "Epoch 65/1000\n",
      "720/720 [==============================] - 0s 115us/step - loss: 1.2487 - acc: 0.4736 - val_loss: 1.1626 - val_acc: 0.5333\n",
      "Epoch 66/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.2296 - acc: 0.4972 - val_loss: 1.1656 - val_acc: 0.5333\n",
      "Epoch 67/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.2335 - acc: 0.4778 - val_loss: 1.1623 - val_acc: 0.5333\n",
      "Epoch 68/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.2023 - acc: 0.5028 - val_loss: 1.1644 - val_acc: 0.5333\n",
      "Epoch 69/1000\n",
      "720/720 [==============================] - 0s 137us/step - loss: 1.2268 - acc: 0.4875 - val_loss: 1.1640 - val_acc: 0.5333\n",
      "Epoch 70/1000\n",
      "720/720 [==============================] - 0s 131us/step - loss: 1.2441 - acc: 0.4764 - val_loss: 1.1650 - val_acc: 0.5333\n",
      "Epoch 71/1000\n",
      "720/720 [==============================] - 0s 114us/step - loss: 1.2268 - acc: 0.4889 - val_loss: 1.1625 - val_acc: 0.5333\n",
      "Epoch 72/1000\n",
      "720/720 [==============================] - 0s 131us/step - loss: 1.2124 - acc: 0.4944 - val_loss: 1.1634 - val_acc: 0.5333\n",
      "Epoch 73/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.2403 - acc: 0.4833 - val_loss: 1.1608 - val_acc: 0.5333\n",
      "Epoch 74/1000\n",
      "720/720 [==============================] - 0s 147us/step - loss: 1.2480 - acc: 0.4667 - val_loss: 1.1593 - val_acc: 0.5333\n",
      "Epoch 75/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.2240 - acc: 0.4903 - val_loss: 1.1600 - val_acc: 0.5333\n",
      "Epoch 76/1000\n",
      "720/720 [==============================] - 0s 121us/step - loss: 1.2265 - acc: 0.4639 - val_loss: 1.1594 - val_acc: 0.5333\n",
      "Epoch 77/1000\n",
      "720/720 [==============================] - 0s 131us/step - loss: 1.2149 - acc: 0.4889 - val_loss: 1.1599 - val_acc: 0.5333\n",
      "Epoch 78/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.2293 - acc: 0.4806 - val_loss: 1.1599 - val_acc: 0.5333\n",
      "Epoch 79/1000\n",
      "720/720 [==============================] - 0s 146us/step - loss: 1.1998 - acc: 0.4833 - val_loss: 1.1593 - val_acc: 0.5333\n",
      "Epoch 80/1000\n",
      "720/720 [==============================] - 0s 137us/step - loss: 1.2032 - acc: 0.5083 - val_loss: 1.1595 - val_acc: 0.5333\n",
      "Epoch 81/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.2337 - acc: 0.4806 - val_loss: 1.1637 - val_acc: 0.5333\n",
      "Epoch 82/1000\n",
      "720/720 [==============================] - 0s 140us/step - loss: 1.2251 - acc: 0.4944 - val_loss: 1.1604 - val_acc: 0.5333\n",
      "Epoch 83/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.2149 - acc: 0.5000 - val_loss: 1.1609 - val_acc: 0.5333\n",
      "Epoch 84/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.2025 - acc: 0.4792 - val_loss: 1.1609 - val_acc: 0.5333\n",
      "Epoch 85/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.2036 - acc: 0.5111 - val_loss: 1.1611 - val_acc: 0.5333\n",
      "Epoch 86/1000\n",
      "720/720 [==============================] - 0s 137us/step - loss: 1.1942 - acc: 0.4819 - val_loss: 1.1608 - val_acc: 0.5333\n",
      "Epoch 87/1000\n",
      "720/720 [==============================] - 0s 157us/step - loss: 1.2238 - acc: 0.4889 - val_loss: 1.1614 - val_acc: 0.5333\n",
      "Epoch 88/1000\n",
      "720/720 [==============================] - 0s 161us/step - loss: 1.2011 - acc: 0.5111 - val_loss: 1.1610 - val_acc: 0.5333\n",
      "Epoch 89/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.2342 - acc: 0.4931 - val_loss: 1.1583 - val_acc: 0.5333\n",
      "Epoch 90/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.2224 - acc: 0.5069 - val_loss: 1.1562 - val_acc: 0.5333\n",
      "Epoch 91/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1904 - acc: 0.4806 - val_loss: 1.1592 - val_acc: 0.5333\n",
      "Epoch 92/1000\n",
      "720/720 [==============================] - 0s 135us/step - loss: 1.2160 - acc: 0.4944 - val_loss: 1.1620 - val_acc: 0.5333\n",
      "Epoch 93/1000\n",
      "720/720 [==============================] - 0s 171us/step - loss: 1.1991 - acc: 0.4931 - val_loss: 1.1604 - val_acc: 0.5333\n",
      "Epoch 94/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.2075 - acc: 0.4972 - val_loss: 1.1602 - val_acc: 0.5333\n",
      "Epoch 95/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.2348 - acc: 0.4958 - val_loss: 1.1593 - val_acc: 0.5333\n",
      "Epoch 96/1000\n",
      "720/720 [==============================] - 0s 121us/step - loss: 1.2511 - acc: 0.4889 - val_loss: 1.1565 - val_acc: 0.5333\n",
      "Epoch 97/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.2194 - acc: 0.4944 - val_loss: 1.1567 - val_acc: 0.5333\n",
      "Epoch 98/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.2132 - acc: 0.4903 - val_loss: 1.1575 - val_acc: 0.5333\n",
      "Epoch 99/1000\n",
      "720/720 [==============================] - 0s 162us/step - loss: 1.1763 - acc: 0.4958 - val_loss: 1.1587 - val_acc: 0.5333\n",
      "Epoch 100/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.2025 - acc: 0.5028 - val_loss: 1.1602 - val_acc: 0.5333\n",
      "Epoch 101/1000\n",
      "720/720 [==============================] - 0s 182us/step - loss: 1.2170 - acc: 0.5069 - val_loss: 1.1597 - val_acc: 0.5333\n",
      "Epoch 102/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1893 - acc: 0.5167 - val_loss: 1.1600 - val_acc: 0.5333\n",
      "Epoch 103/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.1860 - acc: 0.5111 - val_loss: 1.1609 - val_acc: 0.5333\n",
      "Epoch 104/1000\n",
      "720/720 [==============================] - 0s 153us/step - loss: 1.2040 - acc: 0.5028 - val_loss: 1.1590 - val_acc: 0.5333\n",
      "Epoch 105/1000\n",
      "720/720 [==============================] - 0s 148us/step - loss: 1.2038 - acc: 0.4917 - val_loss: 1.1581 - val_acc: 0.5333\n",
      "Epoch 106/1000\n",
      "720/720 [==============================] - 0s 160us/step - loss: 1.1960 - acc: 0.5028 - val_loss: 1.1579 - val_acc: 0.5333\n",
      "Epoch 107/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.2051 - acc: 0.4958 - val_loss: 1.1576 - val_acc: 0.5333\n",
      "Epoch 108/1000\n",
      "720/720 [==============================] - 0s 110us/step - loss: 1.1988 - acc: 0.4986 - val_loss: 1.1563 - val_acc: 0.5333\n",
      "Epoch 109/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.1979 - acc: 0.4931 - val_loss: 1.1555 - val_acc: 0.5333\n",
      "Epoch 110/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.1905 - acc: 0.4958 - val_loss: 1.1553 - val_acc: 0.5333\n",
      "Epoch 111/1000\n",
      "720/720 [==============================] - 0s 155us/step - loss: 1.2022 - acc: 0.4750 - val_loss: 1.1555 - val_acc: 0.5333\n",
      "Epoch 112/1000\n",
      "720/720 [==============================] - 0s 131us/step - loss: 1.2051 - acc: 0.4972 - val_loss: 1.1555 - val_acc: 0.5333\n",
      "Epoch 113/1000\n",
      "720/720 [==============================] - 0s 148us/step - loss: 1.1922 - acc: 0.5000 - val_loss: 1.1547 - val_acc: 0.5333\n",
      "Epoch 114/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1939 - acc: 0.4931 - val_loss: 1.1559 - val_acc: 0.5333\n",
      "Epoch 115/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.2070 - acc: 0.5056 - val_loss: 1.1542 - val_acc: 0.5333\n",
      "Epoch 116/1000\n",
      "720/720 [==============================] - 0s 153us/step - loss: 1.2081 - acc: 0.4958 - val_loss: 1.1557 - val_acc: 0.5333\n",
      "Epoch 117/1000\n",
      "720/720 [==============================] - 0s 156us/step - loss: 1.2060 - acc: 0.4917 - val_loss: 1.1540 - val_acc: 0.5333\n",
      "Epoch 118/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.2047 - acc: 0.5000 - val_loss: 1.1539 - val_acc: 0.5333\n",
      "Epoch 119/1000\n",
      "720/720 [==============================] - 0s 127us/step - loss: 1.1793 - acc: 0.4986 - val_loss: 1.1528 - val_acc: 0.5333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.2006 - acc: 0.4972 - val_loss: 1.1527 - val_acc: 0.5333\n",
      "Epoch 121/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.1960 - acc: 0.4931 - val_loss: 1.1532 - val_acc: 0.5333\n",
      "Epoch 122/1000\n",
      "720/720 [==============================] - ETA: 0s - loss: 1.2068 - acc: 0.492 - 0s 141us/step - loss: 1.2021 - acc: 0.4986 - val_loss: 1.1535 - val_acc: 0.5333\n",
      "Epoch 123/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.2050 - acc: 0.5028 - val_loss: 1.1528 - val_acc: 0.5333\n",
      "Epoch 124/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.1819 - acc: 0.5056 - val_loss: 1.1537 - val_acc: 0.5333\n",
      "Epoch 125/1000\n",
      "720/720 [==============================] - 0s 105us/step - loss: 1.1998 - acc: 0.4972 - val_loss: 1.1531 - val_acc: 0.5333\n",
      "Epoch 126/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1991 - acc: 0.5028 - val_loss: 1.1527 - val_acc: 0.5333\n",
      "Epoch 127/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.2023 - acc: 0.5111 - val_loss: 1.1523 - val_acc: 0.5333\n",
      "Epoch 128/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.2124 - acc: 0.5014 - val_loss: 1.1528 - val_acc: 0.5333\n",
      "Epoch 129/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.2016 - acc: 0.5083 - val_loss: 1.1531 - val_acc: 0.5333\n",
      "Epoch 130/1000\n",
      "720/720 [==============================] - 0s 132us/step - loss: 1.1885 - acc: 0.5222 - val_loss: 1.1541 - val_acc: 0.5333\n",
      "Epoch 131/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.1783 - acc: 0.5097 - val_loss: 1.1545 - val_acc: 0.5333\n",
      "Epoch 132/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1843 - acc: 0.5236 - val_loss: 1.1546 - val_acc: 0.5333\n",
      "Epoch 133/1000\n",
      "720/720 [==============================] - 0s 145us/step - loss: 1.1868 - acc: 0.5042 - val_loss: 1.1533 - val_acc: 0.5333\n",
      "Epoch 134/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1987 - acc: 0.5153 - val_loss: 1.1535 - val_acc: 0.5333\n",
      "Epoch 135/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.2083 - acc: 0.5153 - val_loss: 1.1528 - val_acc: 0.5333\n",
      "Epoch 136/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.1999 - acc: 0.5014 - val_loss: 1.1520 - val_acc: 0.5333\n",
      "Epoch 137/1000\n",
      "720/720 [==============================] - 0s 137us/step - loss: 1.2012 - acc: 0.5056 - val_loss: 1.1518 - val_acc: 0.5333\n",
      "Epoch 138/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.1832 - acc: 0.5097 - val_loss: 1.1520 - val_acc: 0.5333\n",
      "Epoch 139/1000\n",
      "720/720 [==============================] - 0s 110us/step - loss: 1.1732 - acc: 0.5264 - val_loss: 1.1517 - val_acc: 0.5333\n",
      "Epoch 140/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.1551 - acc: 0.5347 - val_loss: 1.1520 - val_acc: 0.5333\n",
      "Epoch 141/1000\n",
      "720/720 [==============================] - 0s 150us/step - loss: 1.1981 - acc: 0.5069 - val_loss: 1.1523 - val_acc: 0.5333\n",
      "Epoch 142/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.1979 - acc: 0.4986 - val_loss: 1.1526 - val_acc: 0.5333\n",
      "Epoch 143/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1693 - acc: 0.5194 - val_loss: 1.1528 - val_acc: 0.5333\n",
      "Epoch 144/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1825 - acc: 0.5000 - val_loss: 1.1527 - val_acc: 0.5333\n",
      "Epoch 145/1000\n",
      "720/720 [==============================] - 0s 128us/step - loss: 1.1869 - acc: 0.5222 - val_loss: 1.1535 - val_acc: 0.5333\n",
      "Epoch 146/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1884 - acc: 0.5111 - val_loss: 1.1529 - val_acc: 0.5333\n",
      "Epoch 147/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1923 - acc: 0.5278 - val_loss: 1.1535 - val_acc: 0.5333\n",
      "Epoch 148/1000\n",
      "720/720 [==============================] - 0s 105us/step - loss: 1.1902 - acc: 0.4986 - val_loss: 1.1517 - val_acc: 0.5333\n",
      "Epoch 149/1000\n",
      "720/720 [==============================] - 0s 135us/step - loss: 1.2034 - acc: 0.5097 - val_loss: 1.1511 - val_acc: 0.5333\n",
      "Epoch 150/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.1757 - acc: 0.4986 - val_loss: 1.1504 - val_acc: 0.5333\n",
      "Epoch 151/1000\n",
      "720/720 [==============================] - 0s 150us/step - loss: 1.1951 - acc: 0.5028 - val_loss: 1.1509 - val_acc: 0.5333\n",
      "Epoch 152/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1904 - acc: 0.5083 - val_loss: 1.1509 - val_acc: 0.5333\n",
      "Epoch 153/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1662 - acc: 0.5278 - val_loss: 1.1516 - val_acc: 0.5333\n",
      "Epoch 154/1000\n",
      "720/720 [==============================] - 0s 132us/step - loss: 1.1711 - acc: 0.5167 - val_loss: 1.1508 - val_acc: 0.5333\n",
      "Epoch 155/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.1908 - acc: 0.5083 - val_loss: 1.1511 - val_acc: 0.5333\n",
      "Epoch 156/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1721 - acc: 0.5306 - val_loss: 1.1517 - val_acc: 0.5333\n",
      "Epoch 157/1000\n",
      "720/720 [==============================] - 0s 142us/step - loss: 1.1884 - acc: 0.5125 - val_loss: 1.1506 - val_acc: 0.5333\n",
      "Epoch 158/1000\n",
      "720/720 [==============================] - 0s 170us/step - loss: 1.1745 - acc: 0.5111 - val_loss: 1.1497 - val_acc: 0.5333\n",
      "Epoch 159/1000\n",
      "720/720 [==============================] - 0s 157us/step - loss: 1.1811 - acc: 0.5125 - val_loss: 1.1508 - val_acc: 0.5333\n",
      "Epoch 160/1000\n",
      "720/720 [==============================] - 0s 127us/step - loss: 1.1942 - acc: 0.5097 - val_loss: 1.1508 - val_acc: 0.5333\n",
      "Epoch 161/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.1871 - acc: 0.4986 - val_loss: 1.1507 - val_acc: 0.5333\n",
      "Epoch 162/1000\n",
      "720/720 [==============================] - 0s 137us/step - loss: 1.2155 - acc: 0.5181 - val_loss: 1.1503 - val_acc: 0.5333\n",
      "Epoch 163/1000\n",
      "720/720 [==============================] - 0s 179us/step - loss: 1.1491 - acc: 0.5292 - val_loss: 1.1514 - val_acc: 0.5333\n",
      "Epoch 164/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.1681 - acc: 0.5278 - val_loss: 1.1504 - val_acc: 0.5333\n",
      "Epoch 165/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.2079 - acc: 0.5097 - val_loss: 1.1507 - val_acc: 0.5333\n",
      "Epoch 166/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1927 - acc: 0.5208 - val_loss: 1.1502 - val_acc: 0.5333\n",
      "Epoch 167/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1897 - acc: 0.5208 - val_loss: 1.1493 - val_acc: 0.5333\n",
      "Epoch 168/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1722 - acc: 0.5194 - val_loss: 1.1495 - val_acc: 0.5333\n",
      "Epoch 169/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.1861 - acc: 0.5292 - val_loss: 1.1500 - val_acc: 0.5333\n",
      "Epoch 170/1000\n",
      "720/720 [==============================] - 0s 158us/step - loss: 1.1852 - acc: 0.5083 - val_loss: 1.1505 - val_acc: 0.5333\n",
      "Epoch 171/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.2021 - acc: 0.5153 - val_loss: 1.1503 - val_acc: 0.5333\n",
      "Epoch 172/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.1688 - acc: 0.5222 - val_loss: 1.1495 - val_acc: 0.5333\n",
      "Epoch 173/1000\n",
      "720/720 [==============================] - 0s 128us/step - loss: 1.2195 - acc: 0.5069 - val_loss: 1.1495 - val_acc: 0.5333\n",
      "Epoch 174/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1704 - acc: 0.5319 - val_loss: 1.1494 - val_acc: 0.5333\n",
      "Epoch 175/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.1954 - acc: 0.5208 - val_loss: 1.1489 - val_acc: 0.5333\n",
      "Epoch 176/1000\n",
      "720/720 [==============================] - 0s 148us/step - loss: 1.1807 - acc: 0.5111 - val_loss: 1.1493 - val_acc: 0.5333\n",
      "Epoch 177/1000\n",
      "720/720 [==============================] - 0s 156us/step - loss: 1.1952 - acc: 0.5181 - val_loss: 1.1495 - val_acc: 0.5333\n",
      "Epoch 178/1000\n",
      "720/720 [==============================] - 0s 109us/step - loss: 1.1794 - acc: 0.5097 - val_loss: 1.1502 - val_acc: 0.5333\n",
      "Epoch 179/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 0s 135us/step - loss: 1.1745 - acc: 0.5097 - val_loss: 1.1496 - val_acc: 0.5333\n",
      "Epoch 180/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1704 - acc: 0.5306 - val_loss: 1.1485 - val_acc: 0.5333\n",
      "Epoch 181/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1890 - acc: 0.5139 - val_loss: 1.1497 - val_acc: 0.5333\n",
      "Epoch 182/1000\n",
      "720/720 [==============================] - 0s 155us/step - loss: 1.1786 - acc: 0.5167 - val_loss: 1.1487 - val_acc: 0.5333\n",
      "Epoch 183/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1906 - acc: 0.5208 - val_loss: 1.1491 - val_acc: 0.5333\n",
      "Epoch 184/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.1886 - acc: 0.5250 - val_loss: 1.1489 - val_acc: 0.5333\n",
      "Epoch 185/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.1714 - acc: 0.5167 - val_loss: 1.1489 - val_acc: 0.5333\n",
      "Epoch 186/1000\n",
      "720/720 [==============================] - 0s 140us/step - loss: 1.1698 - acc: 0.5250 - val_loss: 1.1491 - val_acc: 0.5333\n",
      "Epoch 187/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1885 - acc: 0.5125 - val_loss: 1.1479 - val_acc: 0.5333\n",
      "Epoch 188/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1773 - acc: 0.5153 - val_loss: 1.1472 - val_acc: 0.5333\n",
      "Epoch 189/1000\n",
      "720/720 [==============================] - 0s 109us/step - loss: 1.1920 - acc: 0.5292 - val_loss: 1.1475 - val_acc: 0.5333\n",
      "Epoch 190/1000\n",
      "720/720 [==============================] - 0s 162us/step - loss: 1.1638 - acc: 0.5208 - val_loss: 1.1479 - val_acc: 0.5333\n",
      "Epoch 191/1000\n",
      "720/720 [==============================] - 0s 159us/step - loss: 1.1672 - acc: 0.5153 - val_loss: 1.1471 - val_acc: 0.5333\n",
      "Epoch 192/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.1662 - acc: 0.5194 - val_loss: 1.1477 - val_acc: 0.5333\n",
      "Epoch 193/1000\n",
      "720/720 [==============================] - 0s 109us/step - loss: 1.1841 - acc: 0.5194 - val_loss: 1.1473 - val_acc: 0.5333\n",
      "Epoch 194/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1677 - acc: 0.5278 - val_loss: 1.1476 - val_acc: 0.5333\n",
      "Epoch 195/1000\n",
      "720/720 [==============================] - 0s 92us/step - loss: 1.1921 - acc: 0.5153 - val_loss: 1.1474 - val_acc: 0.5333\n",
      "Epoch 196/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1764 - acc: 0.5181 - val_loss: 1.1478 - val_acc: 0.5333\n",
      "Epoch 197/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1664 - acc: 0.5417 - val_loss: 1.1477 - val_acc: 0.5333\n",
      "Epoch 198/1000\n",
      "720/720 [==============================] - 0s 120us/step - loss: 1.1567 - acc: 0.5194 - val_loss: 1.1478 - val_acc: 0.5333\n",
      "Epoch 199/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1872 - acc: 0.5222 - val_loss: 1.1481 - val_acc: 0.5333\n",
      "Epoch 200/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1818 - acc: 0.5111 - val_loss: 1.1483 - val_acc: 0.5333\n",
      "Epoch 201/1000\n",
      "720/720 [==============================] - 0s 114us/step - loss: 1.1721 - acc: 0.5194 - val_loss: 1.1481 - val_acc: 0.5333\n",
      "Epoch 202/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1756 - acc: 0.5208 - val_loss: 1.1476 - val_acc: 0.5333\n",
      "Epoch 203/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1878 - acc: 0.5181 - val_loss: 1.1487 - val_acc: 0.5333\n",
      "Epoch 204/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1834 - acc: 0.5236 - val_loss: 1.1488 - val_acc: 0.5333\n",
      "Epoch 205/1000\n",
      "720/720 [==============================] - 0s 137us/step - loss: 1.1921 - acc: 0.5222 - val_loss: 1.1485 - val_acc: 0.5333\n",
      "Epoch 206/1000\n",
      "720/720 [==============================] - 0s 159us/step - loss: 1.1755 - acc: 0.5139 - val_loss: 1.1479 - val_acc: 0.5333\n",
      "Epoch 207/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.1804 - acc: 0.5139 - val_loss: 1.1476 - val_acc: 0.5333\n",
      "Epoch 208/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1708 - acc: 0.5389 - val_loss: 1.1475 - val_acc: 0.5333\n",
      "Epoch 209/1000\n",
      "720/720 [==============================] - 0s 157us/step - loss: 1.1692 - acc: 0.5333 - val_loss: 1.1476 - val_acc: 0.5333\n",
      "Epoch 210/1000\n",
      "720/720 [==============================] - 0s 131us/step - loss: 1.1620 - acc: 0.5306 - val_loss: 1.1474 - val_acc: 0.5333\n",
      "Epoch 211/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.1747 - acc: 0.5194 - val_loss: 1.1473 - val_acc: 0.5333\n",
      "Epoch 212/1000\n",
      "720/720 [==============================] - 0s 115us/step - loss: 1.1767 - acc: 0.5222 - val_loss: 1.1471 - val_acc: 0.5333\n",
      "Epoch 213/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1643 - acc: 0.5319 - val_loss: 1.1472 - val_acc: 0.5333\n",
      "Epoch 214/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1740 - acc: 0.5236 - val_loss: 1.1467 - val_acc: 0.5333\n",
      "Epoch 215/1000\n",
      "720/720 [==============================] - 0s 157us/step - loss: 1.1888 - acc: 0.5181 - val_loss: 1.1463 - val_acc: 0.5333\n",
      "Epoch 216/1000\n",
      "720/720 [==============================] - 0s 157us/step - loss: 1.1825 - acc: 0.5306 - val_loss: 1.1472 - val_acc: 0.5333\n",
      "Epoch 217/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.1690 - acc: 0.5306 - val_loss: 1.1467 - val_acc: 0.5333\n",
      "Epoch 218/1000\n",
      "720/720 [==============================] - 0s 146us/step - loss: 1.1764 - acc: 0.5125 - val_loss: 1.1474 - val_acc: 0.5333\n",
      "Epoch 219/1000\n",
      "720/720 [==============================] - 0s 131us/step - loss: 1.1740 - acc: 0.5264 - val_loss: 1.1468 - val_acc: 0.5333\n",
      "Epoch 220/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.1642 - acc: 0.5125 - val_loss: 1.1466 - val_acc: 0.5333\n",
      "Epoch 221/1000\n",
      "720/720 [==============================] - 0s 109us/step - loss: 1.1737 - acc: 0.5222 - val_loss: 1.1461 - val_acc: 0.5333\n",
      "Epoch 222/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1602 - acc: 0.5278 - val_loss: 1.1458 - val_acc: 0.5333\n",
      "Epoch 223/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.1738 - acc: 0.5264 - val_loss: 1.1460 - val_acc: 0.5333\n",
      "Epoch 224/1000\n",
      "720/720 [==============================] - 0s 139us/step - loss: 1.1924 - acc: 0.5361 - val_loss: 1.1459 - val_acc: 0.5333\n",
      "Epoch 225/1000\n",
      "720/720 [==============================] - 0s 135us/step - loss: 1.1823 - acc: 0.5139 - val_loss: 1.1456 - val_acc: 0.5333\n",
      "Epoch 226/1000\n",
      "720/720 [==============================] - 0s 167us/step - loss: 1.1867 - acc: 0.5208 - val_loss: 1.1457 - val_acc: 0.5333\n",
      "Epoch 227/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.1589 - acc: 0.5403 - val_loss: 1.1461 - val_acc: 0.5333\n",
      "Epoch 228/1000\n",
      "720/720 [==============================] - 0s 127us/step - loss: 1.1797 - acc: 0.5208 - val_loss: 1.1464 - val_acc: 0.5333\n",
      "Epoch 229/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1595 - acc: 0.5264 - val_loss: 1.1466 - val_acc: 0.5333\n",
      "Epoch 230/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1691 - acc: 0.5222 - val_loss: 1.1467 - val_acc: 0.5333\n",
      "Epoch 231/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.1642 - acc: 0.5222 - val_loss: 1.1465 - val_acc: 0.5333\n",
      "Epoch 232/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.1862 - acc: 0.5292 - val_loss: 1.1467 - val_acc: 0.5333\n",
      "Epoch 233/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.1579 - acc: 0.5222 - val_loss: 1.1463 - val_acc: 0.5333\n",
      "Epoch 234/1000\n",
      "720/720 [==============================] - 0s 115us/step - loss: 1.1516 - acc: 0.5250 - val_loss: 1.1467 - val_acc: 0.5333\n",
      "Epoch 235/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1657 - acc: 0.5181 - val_loss: 1.1467 - val_acc: 0.5333\n",
      "Epoch 236/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1647 - acc: 0.5264 - val_loss: 1.1465 - val_acc: 0.5333\n",
      "Epoch 237/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1694 - acc: 0.5319 - val_loss: 1.1468 - val_acc: 0.5333\n",
      "Epoch 238/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.1737 - acc: 0.5292 - val_loss: 1.1465 - val_acc: 0.5333\n",
      "Epoch 239/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1721 - acc: 0.5208 - val_loss: 1.1465 - val_acc: 0.5333\n",
      "Epoch 240/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1678 - acc: 0.5208 - val_loss: 1.1459 - val_acc: 0.5333\n",
      "Epoch 241/1000\n",
      "720/720 [==============================] - 0s 90us/step - loss: 1.1796 - acc: 0.5333 - val_loss: 1.1463 - val_acc: 0.5333\n",
      "Epoch 242/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1678 - acc: 0.5278 - val_loss: 1.1462 - val_acc: 0.5333\n",
      "Epoch 243/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1487 - acc: 0.5306 - val_loss: 1.1466 - val_acc: 0.5333\n",
      "Epoch 244/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1768 - acc: 0.5292 - val_loss: 1.1470 - val_acc: 0.5333\n",
      "Epoch 245/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1742 - acc: 0.5236 - val_loss: 1.1467 - val_acc: 0.5333\n",
      "Epoch 246/1000\n",
      "720/720 [==============================] - 0s 93us/step - loss: 1.1540 - acc: 0.5278 - val_loss: 1.1464 - val_acc: 0.5333\n",
      "Epoch 247/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1750 - acc: 0.5278 - val_loss: 1.1464 - val_acc: 0.5333\n",
      "Epoch 248/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1729 - acc: 0.5222 - val_loss: 1.1458 - val_acc: 0.5333\n",
      "Epoch 249/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1752 - acc: 0.5361 - val_loss: 1.1461 - val_acc: 0.5333\n",
      "Epoch 250/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1800 - acc: 0.5278 - val_loss: 1.1460 - val_acc: 0.5333\n",
      "Epoch 251/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1881 - acc: 0.5208 - val_loss: 1.1461 - val_acc: 0.5333\n",
      "Epoch 252/1000\n",
      "720/720 [==============================] - 0s 93us/step - loss: 1.1601 - acc: 0.5236 - val_loss: 1.1464 - val_acc: 0.5333\n",
      "Epoch 253/1000\n",
      "720/720 [==============================] - 0s 113us/step - loss: 1.1596 - acc: 0.5333 - val_loss: 1.1462 - val_acc: 0.5333\n",
      "Epoch 254/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1814 - acc: 0.5208 - val_loss: 1.1457 - val_acc: 0.5333\n",
      "Epoch 255/1000\n",
      "720/720 [==============================] - 0s 88us/step - loss: 1.1742 - acc: 0.5306 - val_loss: 1.1457 - val_acc: 0.5333\n",
      "Epoch 256/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1795 - acc: 0.5250 - val_loss: 1.1456 - val_acc: 0.5333\n",
      "Epoch 257/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1626 - acc: 0.5292 - val_loss: 1.1457 - val_acc: 0.5333\n",
      "Epoch 258/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1825 - acc: 0.5222 - val_loss: 1.1455 - val_acc: 0.5333\n",
      "Epoch 259/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1759 - acc: 0.5222 - val_loss: 1.1454 - val_acc: 0.5333\n",
      "Epoch 260/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1659 - acc: 0.5278 - val_loss: 1.1451 - val_acc: 0.5333\n",
      "Epoch 261/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1912 - acc: 0.5278 - val_loss: 1.1453 - val_acc: 0.5333\n",
      "Epoch 262/1000\n",
      "720/720 [==============================] - 0s 91us/step - loss: 1.1599 - acc: 0.5236 - val_loss: 1.1455 - val_acc: 0.5333\n",
      "Epoch 263/1000\n",
      "720/720 [==============================] - 0s 109us/step - loss: 1.1808 - acc: 0.5306 - val_loss: 1.1454 - val_acc: 0.5333\n",
      "Epoch 264/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1682 - acc: 0.5264 - val_loss: 1.1448 - val_acc: 0.5333\n",
      "Epoch 265/1000\n",
      "720/720 [==============================] - 0s 93us/step - loss: 1.1682 - acc: 0.5278 - val_loss: 1.1450 - val_acc: 0.5333\n",
      "Epoch 266/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1754 - acc: 0.5250 - val_loss: 1.1447 - val_acc: 0.5333\n",
      "Epoch 267/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.1628 - acc: 0.5347 - val_loss: 1.1446 - val_acc: 0.5333\n",
      "Epoch 268/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1769 - acc: 0.5333 - val_loss: 1.1449 - val_acc: 0.5333\n",
      "Epoch 269/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1829 - acc: 0.5264 - val_loss: 1.1448 - val_acc: 0.5333\n",
      "Epoch 270/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1664 - acc: 0.5278 - val_loss: 1.1448 - val_acc: 0.5333\n",
      "Epoch 271/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1722 - acc: 0.5278 - val_loss: 1.1445 - val_acc: 0.5333\n",
      "Epoch 272/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1601 - acc: 0.5292 - val_loss: 1.1442 - val_acc: 0.5333\n",
      "Epoch 273/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1626 - acc: 0.5264 - val_loss: 1.1444 - val_acc: 0.5333\n",
      "Epoch 274/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1705 - acc: 0.5292 - val_loss: 1.1445 - val_acc: 0.5333\n",
      "Epoch 275/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1645 - acc: 0.5250 - val_loss: 1.1446 - val_acc: 0.5333\n",
      "Epoch 276/1000\n",
      "720/720 [==============================] - 0s 133us/step - loss: 1.1525 - acc: 0.5194 - val_loss: 1.1447 - val_acc: 0.5333\n",
      "Epoch 277/1000\n",
      "720/720 [==============================] - 0s 135us/step - loss: 1.1789 - acc: 0.5278 - val_loss: 1.1449 - val_acc: 0.5333\n",
      "Epoch 278/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.1648 - acc: 0.5264 - val_loss: 1.1454 - val_acc: 0.5333\n",
      "Epoch 279/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.1610 - acc: 0.5222 - val_loss: 1.1450 - val_acc: 0.5333\n",
      "Epoch 280/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.1624 - acc: 0.5278 - val_loss: 1.1451 - val_acc: 0.5333\n",
      "Epoch 281/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1710 - acc: 0.5264 - val_loss: 1.1450 - val_acc: 0.5333\n",
      "Epoch 282/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1682 - acc: 0.5292 - val_loss: 1.1448 - val_acc: 0.5333\n",
      "Epoch 283/1000\n",
      "720/720 [==============================] - 0s 87us/step - loss: 1.1655 - acc: 0.5306 - val_loss: 1.1449 - val_acc: 0.5333\n",
      "Epoch 284/1000\n",
      "720/720 [==============================] - 0s 105us/step - loss: 1.1593 - acc: 0.5264 - val_loss: 1.1447 - val_acc: 0.5333\n",
      "Epoch 285/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1520 - acc: 0.5319 - val_loss: 1.1449 - val_acc: 0.5333\n",
      "Epoch 286/1000\n",
      "720/720 [==============================] - 0s 123us/step - loss: 1.1717 - acc: 0.5264 - val_loss: 1.1447 - val_acc: 0.5333\n",
      "Epoch 287/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1739 - acc: 0.5319 - val_loss: 1.1446 - val_acc: 0.5333\n",
      "Epoch 288/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1694 - acc: 0.5292 - val_loss: 1.1451 - val_acc: 0.5333\n",
      "Epoch 289/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1670 - acc: 0.5292 - val_loss: 1.1450 - val_acc: 0.5333\n",
      "Epoch 290/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1760 - acc: 0.5250 - val_loss: 1.1450 - val_acc: 0.5333\n",
      "Epoch 291/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.1514 - acc: 0.5306 - val_loss: 1.1449 - val_acc: 0.5333\n",
      "Epoch 292/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.1707 - acc: 0.5250 - val_loss: 1.1446 - val_acc: 0.5333\n",
      "Epoch 293/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1693 - acc: 0.5347 - val_loss: 1.1444 - val_acc: 0.5333\n",
      "Epoch 294/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1490 - acc: 0.5306 - val_loss: 1.1447 - val_acc: 0.5333\n",
      "Epoch 295/1000\n",
      "720/720 [==============================] - 0s 87us/step - loss: 1.1702 - acc: 0.5319 - val_loss: 1.1445 - val_acc: 0.5333\n",
      "Epoch 296/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1572 - acc: 0.5306 - val_loss: 1.1446 - val_acc: 0.5333\n",
      "Epoch 297/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 0s 149us/step - loss: 1.1736 - acc: 0.5319 - val_loss: 1.1444 - val_acc: 0.5333\n",
      "Epoch 298/1000\n",
      "720/720 [==============================] - 0s 154us/step - loss: 1.1697 - acc: 0.5347 - val_loss: 1.1446 - val_acc: 0.5333\n",
      "Epoch 299/1000\n",
      "720/720 [==============================] - 0s 115us/step - loss: 1.1592 - acc: 0.5278 - val_loss: 1.1446 - val_acc: 0.5333\n",
      "Epoch 300/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1618 - acc: 0.5306 - val_loss: 1.1444 - val_acc: 0.5333\n",
      "Epoch 301/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1509 - acc: 0.5347 - val_loss: 1.1443 - val_acc: 0.5333\n",
      "Epoch 302/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1609 - acc: 0.5319 - val_loss: 1.1443 - val_acc: 0.5333\n",
      "Epoch 303/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.1818 - acc: 0.5333 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 304/1000\n",
      "720/720 [==============================] - 0s 156us/step - loss: 1.1686 - acc: 0.5306 - val_loss: 1.1442 - val_acc: 0.5333\n",
      "Epoch 305/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1722 - acc: 0.5306 - val_loss: 1.1445 - val_acc: 0.5333\n",
      "Epoch 306/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1689 - acc: 0.5306 - val_loss: 1.1443 - val_acc: 0.5333\n",
      "Epoch 307/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1696 - acc: 0.5333 - val_loss: 1.1443 - val_acc: 0.5333\n",
      "Epoch 308/1000\n",
      "720/720 [==============================] - 0s 110us/step - loss: 1.1732 - acc: 0.5278 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 309/1000\n",
      "720/720 [==============================] - 0s 105us/step - loss: 1.1529 - acc: 0.5319 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 310/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1552 - acc: 0.5292 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 311/1000\n",
      "720/720 [==============================] - 0s 142us/step - loss: 1.1731 - acc: 0.5292 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 312/1000\n",
      "720/720 [==============================] - 0s 132us/step - loss: 1.1541 - acc: 0.5347 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 313/1000\n",
      "720/720 [==============================] - 0s 120us/step - loss: 1.1751 - acc: 0.5333 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 314/1000\n",
      "720/720 [==============================] - 0s 133us/step - loss: 1.1829 - acc: 0.5306 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 315/1000\n",
      "720/720 [==============================] - 0s 113us/step - loss: 1.1500 - acc: 0.5375 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 316/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.1567 - acc: 0.5319 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 317/1000\n",
      "720/720 [==============================] - 0s 150us/step - loss: 1.1645 - acc: 0.5333 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 318/1000\n",
      "720/720 [==============================] - 0s 132us/step - loss: 1.1604 - acc: 0.5319 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 319/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.1647 - acc: 0.5222 - val_loss: 1.1435 - val_acc: 0.5333\n",
      "Epoch 320/1000\n",
      "720/720 [==============================] - 0s 128us/step - loss: 1.1765 - acc: 0.5333 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 321/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.1617 - acc: 0.5292 - val_loss: 1.1432 - val_acc: 0.5333\n",
      "Epoch 322/1000\n",
      "720/720 [==============================] - 0s 174us/step - loss: 1.1508 - acc: 0.5264 - val_loss: 1.1432 - val_acc: 0.5333\n",
      "Epoch 323/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.1640 - acc: 0.5319 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 324/1000\n",
      "720/720 [==============================] - 0s 140us/step - loss: 1.1622 - acc: 0.5319 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 325/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1647 - acc: 0.5333 - val_loss: 1.1434 - val_acc: 0.5333\n",
      "Epoch 326/1000\n",
      "720/720 [==============================] - 0s 115us/step - loss: 1.1559 - acc: 0.5347 - val_loss: 1.1435 - val_acc: 0.5333\n",
      "Epoch 327/1000\n",
      "720/720 [==============================] - 0s 118us/step - loss: 1.1588 - acc: 0.5319 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 328/1000\n",
      "720/720 [==============================] - 0s 160us/step - loss: 1.1514 - acc: 0.5347 - val_loss: 1.1435 - val_acc: 0.5333\n",
      "Epoch 329/1000\n",
      "720/720 [==============================] - 0s 170us/step - loss: 1.1584 - acc: 0.5278 - val_loss: 1.1435 - val_acc: 0.5333\n",
      "Epoch 330/1000\n",
      "720/720 [==============================] - 0s 148us/step - loss: 1.1570 - acc: 0.5361 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 331/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.1573 - acc: 0.5375 - val_loss: 1.1431 - val_acc: 0.5333\n",
      "Epoch 332/1000\n",
      "720/720 [==============================] - 0s 128us/step - loss: 1.1508 - acc: 0.5333 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 333/1000\n",
      "720/720 [==============================] - 0s 159us/step - loss: 1.1505 - acc: 0.5333 - val_loss: 1.1435 - val_acc: 0.5333\n",
      "Epoch 334/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.1741 - acc: 0.5333 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 335/1000\n",
      "720/720 [==============================] - 0s 118us/step - loss: 1.1645 - acc: 0.5319 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 336/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1549 - acc: 0.5292 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 337/1000\n",
      "720/720 [==============================] - 0s 132us/step - loss: 1.1410 - acc: 0.5333 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 338/1000\n",
      "720/720 [==============================] - 0s 145us/step - loss: 1.1718 - acc: 0.5319 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 339/1000\n",
      "720/720 [==============================] - 0s 151us/step - loss: 1.1627 - acc: 0.5347 - val_loss: 1.1440 - val_acc: 0.5333\n",
      "Epoch 340/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.1661 - acc: 0.5306 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 341/1000\n",
      "720/720 [==============================] - 0s 140us/step - loss: 1.1613 - acc: 0.5306 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 342/1000\n",
      "720/720 [==============================] - 0s 114us/step - loss: 1.1641 - acc: 0.5319 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 343/1000\n",
      "720/720 [==============================] - 0s 113us/step - loss: 1.1566 - acc: 0.5319 - val_loss: 1.1441 - val_acc: 0.5333\n",
      "Epoch 344/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1695 - acc: 0.5333 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 345/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1688 - acc: 0.5333 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 346/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1682 - acc: 0.5333 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 347/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.1521 - acc: 0.5333 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 348/1000\n",
      "720/720 [==============================] - 0s 123us/step - loss: 1.1538 - acc: 0.5333 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 349/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1624 - acc: 0.5333 - val_loss: 1.1440 - val_acc: 0.5333\n",
      "Epoch 350/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1538 - acc: 0.5319 - val_loss: 1.1444 - val_acc: 0.5333\n",
      "Epoch 351/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1567 - acc: 0.5319 - val_loss: 1.1441 - val_acc: 0.5333\n",
      "Epoch 352/1000\n",
      "720/720 [==============================] - 0s 120us/step - loss: 1.1499 - acc: 0.5319 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 353/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1530 - acc: 0.5333 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 354/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1536 - acc: 0.5319 - val_loss: 1.1438 - val_acc: 0.5333\n",
      "Epoch 355/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1576 - acc: 0.5306 - val_loss: 1.1442 - val_acc: 0.5333\n",
      "Epoch 356/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1612 - acc: 0.5333 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 357/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.1431 - acc: 0.5306 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 358/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1540 - acc: 0.5333 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 359/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1520 - acc: 0.5347 - val_loss: 1.1436 - val_acc: 0.5333\n",
      "Epoch 360/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1578 - acc: 0.5333 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 361/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1606 - acc: 0.5292 - val_loss: 1.1433 - val_acc: 0.5333\n",
      "Epoch 362/1000\n",
      "720/720 [==============================] - 0s 94us/step - loss: 1.1407 - acc: 0.5333 - val_loss: 1.1431 - val_acc: 0.5333\n",
      "Epoch 363/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1684 - acc: 0.5292 - val_loss: 1.1432 - val_acc: 0.5333\n",
      "Epoch 364/1000\n",
      "720/720 [==============================] - 0s 105us/step - loss: 1.1720 - acc: 0.5347 - val_loss: 1.1427 - val_acc: 0.5333\n",
      "Epoch 365/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1580 - acc: 0.5347 - val_loss: 1.1428 - val_acc: 0.5333\n",
      "Epoch 366/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1530 - acc: 0.5292 - val_loss: 1.1429 - val_acc: 0.5333\n",
      "Epoch 367/1000\n",
      "720/720 [==============================] - 0s 121us/step - loss: 1.1638 - acc: 0.5361 - val_loss: 1.1428 - val_acc: 0.5333\n",
      "Epoch 368/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1719 - acc: 0.5319 - val_loss: 1.1428 - val_acc: 0.5333\n",
      "Epoch 369/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1564 - acc: 0.5333 - val_loss: 1.1426 - val_acc: 0.5333\n",
      "Epoch 370/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1595 - acc: 0.5292 - val_loss: 1.1428 - val_acc: 0.5333\n",
      "Epoch 371/1000\n",
      "720/720 [==============================] - 0s 105us/step - loss: 1.1612 - acc: 0.5333 - val_loss: 1.1428 - val_acc: 0.5333\n",
      "Epoch 372/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1431 - acc: 0.5319 - val_loss: 1.1428 - val_acc: 0.5333\n",
      "Epoch 373/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1581 - acc: 0.5333 - val_loss: 1.1425 - val_acc: 0.5333\n",
      "Epoch 374/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1492 - acc: 0.5333 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 375/1000\n",
      "720/720 [==============================] - 0s 93us/step - loss: 1.1571 - acc: 0.5333 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 376/1000\n",
      "720/720 [==============================] - 0s 109us/step - loss: 1.1518 - acc: 0.5319 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 377/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.1489 - acc: 0.5333 - val_loss: 1.1426 - val_acc: 0.5333\n",
      "Epoch 378/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1666 - acc: 0.5319 - val_loss: 1.1425 - val_acc: 0.5333\n",
      "Epoch 379/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1751 - acc: 0.5347 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 380/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1519 - acc: 0.5319 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 381/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1506 - acc: 0.5347 - val_loss: 1.1428 - val_acc: 0.5333\n",
      "Epoch 382/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1627 - acc: 0.5306 - val_loss: 1.1425 - val_acc: 0.5333\n",
      "Epoch 383/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1659 - acc: 0.5306 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 384/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.1475 - acc: 0.5319 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 385/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1532 - acc: 0.5333 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 386/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.1526 - acc: 0.5333 - val_loss: 1.1421 - val_acc: 0.5333\n",
      "Epoch 387/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1525 - acc: 0.5319 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 388/1000\n",
      "720/720 [==============================] - 0s 109us/step - loss: 1.1533 - acc: 0.5278 - val_loss: 1.1425 - val_acc: 0.5333\n",
      "Epoch 389/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1696 - acc: 0.5319 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 390/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1698 - acc: 0.5333 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 391/1000\n",
      "720/720 [==============================] - 0s 112us/step - loss: 1.1509 - acc: 0.5306 - val_loss: 1.1425 - val_acc: 0.5333\n",
      "Epoch 392/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.1599 - acc: 0.5319 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 393/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1521 - acc: 0.5319 - val_loss: 1.1424 - val_acc: 0.5333\n",
      "Epoch 394/1000\n",
      "720/720 [==============================] - 0s 120us/step - loss: 1.1543 - acc: 0.5306 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 395/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.1609 - acc: 0.5333 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 396/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.1511 - acc: 0.5347 - val_loss: 1.1421 - val_acc: 0.5333\n",
      "Epoch 397/1000\n",
      "720/720 [==============================] - 0s 116us/step - loss: 1.1725 - acc: 0.5292 - val_loss: 1.1421 - val_acc: 0.5333\n",
      "Epoch 398/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.1523 - acc: 0.5333 - val_loss: 1.1421 - val_acc: 0.5333\n",
      "Epoch 399/1000\n",
      "720/720 [==============================] - 0s 93us/step - loss: 1.1725 - acc: 0.5278 - val_loss: 1.1420 - val_acc: 0.5333\n",
      "Epoch 400/1000\n",
      "720/720 [==============================] - 0s 90us/step - loss: 1.1601 - acc: 0.5319 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 401/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1638 - acc: 0.5319 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 402/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1635 - acc: 0.5319 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 403/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1575 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 404/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1618 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 405/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1572 - acc: 0.5319 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 406/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.1647 - acc: 0.5319 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 407/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1572 - acc: 0.5319 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 408/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1490 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 409/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1589 - acc: 0.5319 - val_loss: 1.1420 - val_acc: 0.5333\n",
      "Epoch 410/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1583 - acc: 0.5319 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 411/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1395 - acc: 0.5333 - val_loss: 1.1420 - val_acc: 0.5333\n",
      "Epoch 412/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1403 - acc: 0.5347 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 413/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1655 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 414/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1528 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 415/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 0s 101us/step - loss: 1.1522 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 416/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1458 - acc: 0.5319 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 417/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.1621 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 418/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1434 - acc: 0.5347 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 419/1000\n",
      "720/720 [==============================] - 0s 92us/step - loss: 1.1529 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 420/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1547 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 421/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1623 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 422/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1580 - acc: 0.5319 - val_loss: 1.1421 - val_acc: 0.5333\n",
      "Epoch 423/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1562 - acc: 0.5333 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 424/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1602 - acc: 0.5333 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 425/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1565 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 426/1000\n",
      "720/720 [==============================] - 0s 105us/step - loss: 1.1492 - acc: 0.5306 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 427/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1570 - acc: 0.5361 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 428/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1488 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 429/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1606 - acc: 0.5319 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 430/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1524 - acc: 0.5319 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 431/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1553 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 432/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1450 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 433/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1650 - acc: 0.5306 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 434/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1522 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 435/1000\n",
      "720/720 [==============================] - 0s 90us/step - loss: 1.1590 - acc: 0.5319 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 436/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.1559 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 437/1000\n",
      "720/720 [==============================] - 0s 93us/step - loss: 1.1577 - acc: 0.5319 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 438/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1480 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 439/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1674 - acc: 0.5306 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 440/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1569 - acc: 0.5306 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 441/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1521 - acc: 0.5319 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 442/1000\n",
      "720/720 [==============================] - 0s 91us/step - loss: 1.1571 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 443/1000\n",
      "720/720 [==============================] - 0s 94us/step - loss: 1.1615 - acc: 0.5319 - val_loss: 1.1413 - val_acc: 0.5333\n",
      "Epoch 444/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1520 - acc: 0.5347 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 445/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1417 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 446/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1639 - acc: 0.5319 - val_loss: 1.1412 - val_acc: 0.5333\n",
      "Epoch 447/1000\n",
      "720/720 [==============================] - 0s 123us/step - loss: 1.1679 - acc: 0.5333 - val_loss: 1.1410 - val_acc: 0.5333\n",
      "Epoch 448/1000\n",
      "720/720 [==============================] - 0s 103us/step - loss: 1.1575 - acc: 0.5319 - val_loss: 1.1412 - val_acc: 0.5333\n",
      "Epoch 449/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1597 - acc: 0.5333 - val_loss: 1.1411 - val_acc: 0.5333\n",
      "Epoch 450/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1715 - acc: 0.5333 - val_loss: 1.1411 - val_acc: 0.5333\n",
      "Epoch 451/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1501 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 452/1000\n",
      "720/720 [==============================] - 0s 91us/step - loss: 1.1561 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 453/1000\n",
      "720/720 [==============================] - 0s 97us/step - loss: 1.1336 - acc: 0.5319 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 454/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1554 - acc: 0.5347 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 455/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1429 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 456/1000\n",
      "720/720 [==============================] - 0s 91us/step - loss: 1.1505 - acc: 0.5319 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 457/1000\n",
      "720/720 [==============================] - 0s 104us/step - loss: 1.1418 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 458/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1588 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 459/1000\n",
      "720/720 [==============================] - 0s 96us/step - loss: 1.1540 - acc: 0.5319 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 460/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1479 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 461/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.1599 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 462/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1444 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 463/1000\n",
      "720/720 [==============================] - 0s 147us/step - loss: 1.1493 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 464/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1418 - acc: 0.5333 - val_loss: 1.1421 - val_acc: 0.5333\n",
      "Epoch 465/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1414 - acc: 0.5333 - val_loss: 1.1421 - val_acc: 0.5333\n",
      "Epoch 466/1000\n",
      "720/720 [==============================] - 0s 100us/step - loss: 1.1476 - acc: 0.5333 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 467/1000\n",
      "720/720 [==============================] - 0s 102us/step - loss: 1.1569 - acc: 0.5347 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 468/1000\n",
      "720/720 [==============================] - 0s 92us/step - loss: 1.1487 - acc: 0.5333 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 469/1000\n",
      "720/720 [==============================] - 0s 118us/step - loss: 1.1555 - acc: 0.5333 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 470/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1566 - acc: 0.5333 - val_loss: 1.1420 - val_acc: 0.5333\n",
      "Epoch 471/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1465 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 472/1000\n",
      "720/720 [==============================] - 0s 133us/step - loss: 1.1547 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 473/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1532 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 474/1000\n",
      "720/720 [==============================] - 0s 106us/step - loss: 1.1551 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 475/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.1498 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 476/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1399 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 477/1000\n",
      "720/720 [==============================] - 0s 99us/step - loss: 1.1618 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 478/1000\n",
      "720/720 [==============================] - 0s 98us/step - loss: 1.1410 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 479/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1597 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 480/1000\n",
      "720/720 [==============================] - 0s 151us/step - loss: 1.1468 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 481/1000\n",
      "720/720 [==============================] - 0s 227us/step - loss: 1.1525 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 482/1000\n",
      "720/720 [==============================] - 0s 176us/step - loss: 1.1403 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 483/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1540 - acc: 0.5319 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 484/1000\n",
      "720/720 [==============================] - 0s 121us/step - loss: 1.1493 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 485/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.1495 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 486/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.1559 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 487/1000\n",
      "720/720 [==============================] - 0s 156us/step - loss: 1.1443 - acc: 0.5319 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 488/1000\n",
      "720/720 [==============================] - 0s 124us/step - loss: 1.1619 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 489/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.1478 - acc: 0.5347 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 490/1000\n",
      "720/720 [==============================] - 0s 119us/step - loss: 1.1481 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 491/1000\n",
      "720/720 [==============================] - 0s 156us/step - loss: 1.1547 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 492/1000\n",
      "720/720 [==============================] - 0s 142us/step - loss: 1.1644 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 493/1000\n",
      "720/720 [==============================] - 0s 146us/step - loss: 1.1485 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 494/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1465 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 495/1000\n",
      "720/720 [==============================] - 0s 120us/step - loss: 1.1462 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 496/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1451 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 497/1000\n",
      "720/720 [==============================] - 0s 140us/step - loss: 1.1409 - acc: 0.5333 - val_loss: 1.1420 - val_acc: 0.5333\n",
      "Epoch 498/1000\n",
      "720/720 [==============================] - 0s 150us/step - loss: 1.1604 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 499/1000\n",
      "720/720 [==============================] - 0s 135us/step - loss: 1.1450 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 500/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.1563 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 501/1000\n",
      "720/720 [==============================] - 0s 138us/step - loss: 1.1540 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 502/1000\n",
      "720/720 [==============================] - 0s 163us/step - loss: 1.1506 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 503/1000\n",
      "720/720 [==============================] - 0s 152us/step - loss: 1.1582 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 504/1000\n",
      "720/720 [==============================] - 0s 140us/step - loss: 1.1557 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 505/1000\n",
      "720/720 [==============================] - 0s 115us/step - loss: 1.1447 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 506/1000\n",
      "720/720 [==============================] - 0s 149us/step - loss: 1.1478 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 507/1000\n",
      "720/720 [==============================] - 0s 171us/step - loss: 1.1646 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 508/1000\n",
      "720/720 [==============================] - 0s 152us/step - loss: 1.1439 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 509/1000\n",
      "720/720 [==============================] - 0s 134us/step - loss: 1.1578 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 510/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.1482 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 511/1000\n",
      "720/720 [==============================] - 0s 168us/step - loss: 1.1428 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 512/1000\n",
      "720/720 [==============================] - 0s 168us/step - loss: 1.1481 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 513/1000\n",
      "720/720 [==============================] - 0s 147us/step - loss: 1.1510 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 514/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.1519 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 515/1000\n",
      "720/720 [==============================] - 0s 129us/step - loss: 1.1522 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 516/1000\n",
      "720/720 [==============================] - 0s 130us/step - loss: 1.1330 - acc: 0.5319 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 517/1000\n",
      "720/720 [==============================] - 0s 163us/step - loss: 1.1430 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 518/1000\n",
      "720/720 [==============================] - 0s 161us/step - loss: 1.1573 - acc: 0.5333 - val_loss: 1.1413 - val_acc: 0.5333\n",
      "Epoch 519/1000\n",
      "720/720 [==============================] - 0s 162us/step - loss: 1.1566 - acc: 0.5333 - val_loss: 1.1412 - val_acc: 0.5333\n",
      "Epoch 520/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.1453 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 521/1000\n",
      "720/720 [==============================] - 0s 110us/step - loss: 1.1478 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 522/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.1515 - acc: 0.5333 - val_loss: 1.1413 - val_acc: 0.5333\n",
      "Epoch 523/1000\n",
      "720/720 [==============================] - 0s 148us/step - loss: 1.1518 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 524/1000\n",
      "720/720 [==============================] - 0s 154us/step - loss: 1.1500 - acc: 0.5333 - val_loss: 1.1413 - val_acc: 0.5333\n",
      "Epoch 525/1000\n",
      "720/720 [==============================] - 0s 145us/step - loss: 1.1540 - acc: 0.5333 - val_loss: 1.1414 - val_acc: 0.5333\n",
      "Epoch 526/1000\n",
      "720/720 [==============================] - 0s 136us/step - loss: 1.1496 - acc: 0.5333 - val_loss: 1.1413 - val_acc: 0.5333\n",
      "Epoch 527/1000\n",
      "720/720 [==============================] - 0s 143us/step - loss: 1.1482 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 528/1000\n",
      "720/720 [==============================] - 0s 145us/step - loss: 1.1380 - acc: 0.5333 - val_loss: 1.1415 - val_acc: 0.5333\n",
      "Epoch 529/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.1490 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 530/1000\n",
      "720/720 [==============================] - 0s 141us/step - loss: 1.1473 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 531/1000\n",
      "720/720 [==============================] - 0s 144us/step - loss: 1.1518 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 532/1000\n",
      "720/720 [==============================] - 0s 148us/step - loss: 1.1473 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 533/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 0s 132us/step - loss: 1.1514 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 534/1000\n",
      "720/720 [==============================] - 0s 157us/step - loss: 1.1470 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 535/1000\n",
      "720/720 [==============================] - 0s 125us/step - loss: 1.1544 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 536/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1408 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 537/1000\n",
      "720/720 [==============================] - 0s 126us/step - loss: 1.1575 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 538/1000\n",
      "720/720 [==============================] - 0s 122us/step - loss: 1.1526 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 539/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1435 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 540/1000\n",
      "720/720 [==============================] - 0s 95us/step - loss: 1.1476 - acc: 0.5333 - val_loss: 1.1418 - val_acc: 0.5333\n",
      "Epoch 541/1000\n",
      "720/720 [==============================] - 0s 101us/step - loss: 1.1357 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 542/1000\n",
      "720/720 [==============================] - 0s 111us/step - loss: 1.1480 - acc: 0.5333 - val_loss: 1.1419 - val_acc: 0.5333\n",
      "Epoch 543/1000\n",
      "720/720 [==============================] - 0s 113us/step - loss: 1.1488 - acc: 0.5333 - val_loss: 1.1420 - val_acc: 0.5333\n",
      "Epoch 544/1000\n",
      "720/720 [==============================] - 0s 107us/step - loss: 1.1508 - acc: 0.5333 - val_loss: 1.1417 - val_acc: 0.5333\n",
      "Epoch 545/1000\n",
      "720/720 [==============================] - 0s 121us/step - loss: 1.1534 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 546/1000\n",
      "720/720 [==============================] - 0s 108us/step - loss: 1.1494 - acc: 0.5333 - val_loss: 1.1416 - val_acc: 0.5333\n",
      "Epoch 547/1000\n",
      "720/720 [==============================] - 0s 117us/step - loss: 1.1566 - acc: 0.5333 - val_loss: 1.1413 - val_acc: 0.5333\n"
     ]
    }
   ],
   "source": [
    "validation_data_split = 0.2\n",
    "num_epochs = 1000\n",
    "model_batch_size = 128\n",
    "early_patience = 100\n",
    "\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=0, patience=early_patience, mode='min')\n",
    "\n",
    "# Read Dataset\n",
    "dataset = pd.read_csv('training.csv')\n",
    "\n",
    "# Process Dataset\n",
    "processedData, processedLabel = processData(dataset)\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [earlystopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, STATUS_OK, tpe, fmin, Trials\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ITERATION = 0\n",
    "\n",
    "def hypp_search_objective(hypp):\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    \n",
    "    hypp.update(hypp.pop('num_hidden'))\n",
    "    hypp['hidden_layer_nodes'] = [int(x) for x in hypp['hidden_layer_nodes']]\n",
    "    \n",
    "    model = get_model(hypp)\n",
    "    \n",
    "    history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split = validation_data_split\n",
    "                    , epochs = num_epochs\n",
    "                    , batch_size = model_batch_size\n",
    "                    , callbacks = [earlystopping_cb]\n",
    "                    , verbose = False\n",
    "                   )\n",
    "    best_acc = np.max(history.history['val_acc'])\n",
    "    train_acc = np.max(history.history['acc'])\n",
    "    loss = 1 - best_acc\n",
    "    \n",
    "    return {'loss': loss, 'best_acc': best_acc, 'train_acc': train_acc,\n",
    "            'params': hypp, 'iteration': ITERATION, 'status': STATUS_OK}\n",
    "\n",
    "def class_accuracy(pred_score, y_true):\n",
    "    \n",
    "    \n",
    "    CM = confusion_matrix(\n",
    "        np.argmax(y_true,axis=1),\n",
    "        np.argmax(pred_score,axis=1)\n",
    "    )\n",
    "    return np.diagonal(CM / CM.sum(axis=1))\n",
    "\n",
    "def model_test(model,data):\n",
    "    \n",
    "    hist = model.fit(data['X_tr'],\n",
    "              data['y_tr'],\n",
    "              validation_split = 0.2,\n",
    "              epochs = num_epochs,\n",
    "              batch_size = model_batch_size,\n",
    "              callbacks = [earlystopping_cb],\n",
    "              verbose = False)\n",
    "    \n",
    "    pred_train = model.predict(data['X_tr'])\n",
    "    pred_test = model.predict(data['X_ts'])\n",
    "    \n",
    "    return {\n",
    "        'train': class_accuracy(data['y_tr'], pred_train),\n",
    "        'test': class_accuracy(data['y_ts'], pred_test),\n",
    "        'hist': hist\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### domain space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    'activation': hp.choice('activation', ['tanh', 'relu', 'elu', 'selu']),\n",
    "    'num_hidden': hp.choice('num_hidden', [{'num_hidden': 1, 'hidden_layer_nodes': [hp.qnormal('hidden_11_size', 256, 10, 1)]}, \n",
    "                                           {'num_hidden': 2, 'hidden_layer_nodes': [hp.qnormal('hidden_12_size', 256, 10, 1), hp.qnormal('hidden_22_size', 128, 10, 1)]},\n",
    "                                           {'num_hidden': 3, 'hidden_layer_nodes': [hp.qnormal('hidden_13_size', 256, 10, 1), hp.qnormal('hidden_23_size', 128, 10, 1), hp.qnormal('hidden_2_size', 64, 5, 1)]}]),\n",
    "    'drop_out': hp.uniform('drop_out', 0.0, 0.9),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)) #more frequent sampling for smaller values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = fmin(fn = hypp_search_objective, space = space, algo = tpe.suggest,\n",
    "            max_evals = 5, trials = bayes_trials, rstate = np.random.RandomState(573))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_time': datetime.datetime(2018, 9, 16, 18, 33, 4, 593000),\n",
       " 'exp_key': None,\n",
       " 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'idxs': {'activation': [2],\n",
       "   'drop_out': [2],\n",
       "   'hidden_11_size': [],\n",
       "   'hidden_12_size': [2],\n",
       "   'hidden_13_size': [],\n",
       "   'hidden_22_size': [2],\n",
       "   'hidden_23_size': [],\n",
       "   'hidden_2_size': [],\n",
       "   'learning_rate': [2],\n",
       "   'num_hidden': [2]},\n",
       "  'tid': 2,\n",
       "  'vals': {'activation': [3],\n",
       "   'drop_out': [0.14026717737669409],\n",
       "   'hidden_11_size': [],\n",
       "   'hidden_12_size': [257.0],\n",
       "   'hidden_13_size': [],\n",
       "   'hidden_22_size': [122.0],\n",
       "   'hidden_23_size': [],\n",
       "   'hidden_2_size': [],\n",
       "   'learning_rate': [0.0012242704296310323],\n",
       "   'num_hidden': [1]},\n",
       "  'workdir': None},\n",
       " 'owner': None,\n",
       " 'refresh_time': datetime.datetime(2018, 9, 16, 18, 33, 42, 811000),\n",
       " 'result': {'best_acc': 0.9444444497426351,\n",
       "  'iteration': 3,\n",
       "  'loss': 0.055555550257364916,\n",
       "  'params': {'activation': 'selu',\n",
       "   'drop_out': 0.14026717737669409,\n",
       "   'hidden_layer_nodes': [257, 122],\n",
       "   'learning_rate': 0.0012242704296310323,\n",
       "   'num_hidden': 2},\n",
       "  'status': 'ok',\n",
       "  'train_acc': 0.8222222235467699},\n",
       " 'spec': None,\n",
       " 'state': 2,\n",
       " 'tid': 2,\n",
       " 'version': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(bayes_trials.trials, key=lambda x: x['result']['best_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Google Colaboratory found that\n",
    "```\n",
    "{\n",
    "    'drop_out':  0.339,\n",
    "    'hidden_layer_nodes': [245, 154, 67],\n",
    "    'num_hidden': 3,\n",
    "    'activation': 'tanh',\n",
    "    'learning_rate': 0.00815\n",
    "}```\n",
    "is the best option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how train and test scores depend on Dropout and learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_layer_nodes': [245, 154, 67],\n",
    "    'num_hidden': 3,\n",
    "    'activation': 'tanh',\n",
    "    'learning_rate': 0.00815,\n",
    "    'drop_out': 0.339\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./training.csv')\n",
    "data_test = pd.read_csv('./testing.csv')\n",
    "\n",
    "X_tr, y_tr = processData(data_train)\n",
    "X_ts, y_ts = processData(data_test)\n",
    "\n",
    "data = {\n",
    "    'X_tr': X_tr,\n",
    "    'X_ts': X_ts,\n",
    "    'y_tr': y_tr,\n",
    "    'y_ts': y_ts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(params)\n",
    "model.save_weights('./init_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8e486491d54da8814c9e09f725f9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolterlw/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/wolterlw/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dropout_test = []\n",
    "\n",
    "params_wo_dropout = params.copy()\n",
    "del params_wo_dropout['drop_out']\n",
    "\n",
    "for dropout in tqdm(np.linspace(0,0.9,num=20)):\n",
    "    model = get_model(dict({'drop_out': dropout}, **params_wo_dropout))\n",
    "    model.load_weights('./init_weights.hdf5')\n",
    "    dropout_test.append(\n",
    "        model_test(model, data)   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAADSCAYAAAAPI/KSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVHn4JkxtMDUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd5b5d4f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropout_test = {\n",
    "    'test': np.r_[[x['test'] for x in dropout_test]],\n",
    "    'train': np.r_[[x['train'] for x in dropout_test]]\n",
    "}\n",
    "\n",
    "cols = ['other', 'fizz', 'buzz', 'fizzbuzz']\n",
    "index = np.linspace(0,1,20)\n",
    "\n",
    "do_train = pd.DataFrame(dropout_test['train'],columns=cols, index=index).fillna(0)\n",
    "do_test = pd.DataFrame(dropout_test['test'],columns=cols, index=index).fillna(0)\n",
    "\n",
    "f,ax = plt.subplots(1,4,figsize=(15,3))\n",
    "\n",
    "for i,c in enumerate(cols):\n",
    "    ax[i].set_title(c)\n",
    "    do_train[c].plot(label='train', ax=ax[i])\n",
    "    do_test[c].plot(label='test', ax=ax[i])\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb93a312aa52429aa8030ac3b57260b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolterlw/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/wolterlw/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "algo_test = []\n",
    "\n",
    "for algo in tqdm(['SGD','RMSprop','Adam','Nadam']):\n",
    "    model = get_model(params, optim_algo=algo)\n",
    "    model.load_weights('./init_weights.hdf5')\n",
    "    algo_test.append(\n",
    "        model_test(model, data)   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo_test = {\n",
    "    'test': np.r_[[x['test'] for x in algo_test]],\n",
    "    'train': np.r_[[x['train'] for x in algo_test]],\n",
    "    'epochs_train': np.r_[[len(x['hist'].history['acc']) for x in algo_test]]\n",
    "}\n",
    "\n",
    "cols = ['other', 'fizz', 'buzz', 'fizzbuzz']\n",
    "index = ['SGD','RMSprop','Adam','Nadam']\n",
    "\n",
    "algo_train = pd.DataFrame(algo_test['train'],columns=cols, index=index).fillna(0)\n",
    "algo_train['n_epochs'] = algo_test['epochs_train']\n",
    "algo_test = pd.DataFrame(algo_test['test'],columns=cols, index=index).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo_test.columns = [x+'_test' for x in algo_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD        0.133333\n",
       "RMSprop    0.986487\n",
       "Adam       0.987118\n",
       "Nadam      0.970924\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_train.drop('n_epochs',axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD        0.132500\n",
       "RMSprop    1.000000\n",
       "Adam       0.981125\n",
       "Nadam      0.995370\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other_test</th>\n",
       "      <th>fizz_test</th>\n",
       "      <th>buzz_test</th>\n",
       "      <th>fizzbuzz_test</th>\n",
       "      <th>other</th>\n",
       "      <th>fizz</th>\n",
       "      <th>buzz</th>\n",
       "      <th>fizzbuzz</th>\n",
       "      <th>n_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSprop</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987552</td>\n",
       "      <td>0.983122</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adam</th>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nadam</th>\n",
       "      <td>0.981481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         other_test  fizz_test  buzz_test  fizzbuzz_test     other      fizz  \\\n",
       "SGD        0.530000   0.000000        0.0            0.0  0.533333  0.000000   \n",
       "RMSprop    1.000000   1.000000        1.0            1.0  0.987552  0.983122   \n",
       "Adam       0.962963   0.961538        1.0            1.0  0.985507  0.962963   \n",
       "Nadam      0.981481   1.000000        1.0            1.0  0.955645  1.000000   \n",
       "\n",
       "             buzz  fizzbuzz  n_epochs  \n",
       "SGD      0.000000  0.000000       108  \n",
       "RMSprop  0.991667  0.983607       808  \n",
       "Adam     1.000000  1.000000       858  \n",
       "Nadam    0.944444  0.983607       690  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([algo_test,algo_train],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing lower learning rate for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'drop_out': 0.339,\n",
       " 'hidden_layer_nodes': [245, 154, 67],\n",
       " 'learning_rate': 0.00815,\n",
       " 'num_hidden': 3}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolterlw/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/wolterlw/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "params_lr = params.copy()\n",
    "params_lr['learning_rate'] = 1e-2\n",
    "\n",
    "model = get_model(params_lr, optim_algo='SGD')\n",
    "model.load_weights('./init_weights.hdf5')\n",
    "res = model_test(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Training and Validation Graphs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "df.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Testing Accuracy [Software 2.0]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decodeLabel(encodedLabel):\n",
    "    if encodedLabel == 0:\n",
    "        return \"other\"\n",
    "    elif encodedLabel == 1:\n",
    "        return \"fizz\"\n",
    "    elif encodedLabel == 2:\n",
    "        return \"buzz\"\n",
    "    elif encodedLabel == 3:\n",
    "        return \"fizzbuzz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(params)\n",
    "model_test(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "testData = pd.read_csv('testing.csv')\n",
    "\n",
    "processedTestData  = encodeData(testData['input'].values)\n",
    "processedTestLabel = encodeLabel(testData['label'].values)\n",
    "predictedTestLabel = []\n",
    "\n",
    "for i,j in zip(processedTestData,processedTestLabel):\n",
    "    y = model.predict(np.array(i).reshape(-1,10))\n",
    "    predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "    \n",
    "    if j.argmax() == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))\n",
    "\n",
    "output = {}\n",
    "output[\"input\"] = testData['input']\n",
    "output[\"label\"] = testData['label']\n",
    "output[\"predicted_label\"] = predictedTestLabel\n",
    "\n",
    "opdf = pd.DataFrame(output)\n",
    "opdf.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
